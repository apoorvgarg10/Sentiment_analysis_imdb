{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment1_apoorvgarg_db.ipynb","provenance":[{"file_id":"1wv9a1oEkKjx9FAk0nZNxvf7UjoZ-dBxc","timestamp":1633058967517}],"collapsed_sections":[],"mount_file_id":"1wv9a1oEkKjx9FAk0nZNxvf7UjoZ-dBxc","authorship_tag":"ABX9TyPSsAF1RmqVxykg06SkpUuD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4f9929c98c03455786efff8b9ae31408":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_60e8884e68304c6ca2e1769ceb5b1570","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_68f84527e5124dcbaa44315df71ddd28","IPY_MODEL_a4dc97ec774b40acacd57594f672b5bd","IPY_MODEL_78a0a9b35c6e4758a86cb1d93b7d3f84"]}},"60e8884e68304c6ca2e1769ceb5b1570":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"68f84527e5124dcbaa44315df71ddd28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_51a3fc0e6e5f492d9a3a9ff32f3b0034","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1e329cd317394d55bc51c682d7c4aa4c"}},"a4dc97ec774b40acacd57594f672b5bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_14568195bfe246f5978d6b761a3f0335","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":263273408,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":263273408,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1fb90a44d8f243ebbc7270afc52cb74f"}},"78a0a9b35c6e4758a86cb1d93b7d3f84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0fac08a2358446fe8f9427a25d7f6c39","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 251M/251M [00:09&lt;00:00, 27.5MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_17301e970d8e4f259cee2cc4e9ebcad5"}},"51a3fc0e6e5f492d9a3a9ff32f3b0034":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1e329cd317394d55bc51c682d7c4aa4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"14568195bfe246f5978d6b761a3f0335":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1fb90a44d8f243ebbc7270afc52cb74f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0fac08a2358446fe8f9427a25d7f6c39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"17301e970d8e4f259cee2cc4e9ebcad5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_9FayBe6fbFn"},"source":["I have used two different pretrained language models fron huggingface to train on this dataset. \n","\n","*   bert-base-cased pretrained for the downstream task: sentence classification. The name for this model in HuggingFace repo is: **BertForSequenceClassification**. \n","*   distillbert-base-cased. To this model, I have added 2 linear layers separated by a dropout layer and a RelU non-linearity.\n","\n","I reveived the following metrics on the **test dataset**:\n","\n","###  **BertForSequenceClassification**: \n",">  1. Precision =  0.9159170903402425\n",">  2. Recall =  0.9349301397205588\n",">  3. F1 score =  0.9253259581193203\n",">  4. Accuracy = 0.9244\n","\n","###  **Sentence Classifier: distillbert-base-cased + 2 Linear Layers + Dropout + ReLU**:\n",">  1. Precision =  0.9021823850350741\n",">  2. Recall =  0.9241516966067864\n",">  3. F1 score =  0.9130349043581149\n",">  4. Accuracy = 0.9118\n","\n","Since both models achieve the required metric of 0.90, I am submitting the distillbert model as my submission. But, I am also attaching the notebook,model and config files for the BertForSequenceClassification in the extras folder, since it has better score and has helped me in making some decisions for my final submission.\n","\n","Performace metrics of Sentence Classifier on the Validation Dataset:\n",">  1. Precision =  0.9033646322378717\n",">  2. Recall =  0.9184566428003182\n",">  3. F1 score =  0.9108481262327416\n",">  4. Accuracy = 0.9096\n","\n","Performace metrics of Sentence Classifier on the entire Training Dataset:\n",">  1. Precision =  0.9993494144730257\n",">  2. Recall =  0.9993994294579851\n",">  3. F1 score =  0.9993744213397393\n",">  4. Accuracy = 0.999375\n","\n",">**Note: I have chosen distillbert because of the ease of training(provided the constraint that the runtime ggets disconnected after ~6+ hours of training and the I wanted the loss to start converging. For distillbert and linear layer combination, I saw that the loss was converging after 5th epoch to the order of 1e-4. I could only train ~2-3 epochs for bert base model, but for distillbert, I could train and infer on more than 5 epochs, an was sure on the loss convergence. Distillbert also does not have token_type_ids encoding for the tokeniser.)**\n","\n","\n","\n","\n","The entire process can be divided into: \n","# 1. **Dataloader design**: \n"," \n"," I read the csv file as pandas dataframes and loaded them in the pytorch dataloader class as custom datasets. This also involves the design of tokenizer.\n","> **Tokeniser Design :** I chose the cased version of the tokeniser to incorporate extra information in reviews like \"BAD\" instead of \"bad\". \n","> I chose a max-length of 256 because the average review length was 231.33 words on the training set.\n","> I chose a batch size of 16 (32 was giving an out of memory error.)\n","> To incorporate linear layer on CLS token, use_special_tokens=True.\n","\n","# 2. **Classifier Design**:\n","\n","* I have tried both, bert-base and Distillbert as the language model for\n","tokenizing and finetuning purpose. For two epochs, the F1 score on bert base cased was 0.92, while the F1 score was 0.9 on the Distilled Bert model. However, the error was still converging for both models. It was found that Distilled Bert with two Linear layers took approximately 30 minutes for one epoch of training and validation on 1600 samples, while BertForSequenceClassification took 150 minutes to train. This is a 4.44% upside in performance for 400% increase in training time. This is reason Distilled Bert has been chosen as the Language Model for fine tuning.\n","\n","* The CLS token of the pretrained Language Model is the forst value in the encodings, and has a size of 16 X 768 for each batch (i.e. a size of 768 for each review if squeezed). I have chosen 1 extra hidden linear layer with a size of 20, a dropout of 0.2, and a ReLU non-linearity. (I followed: Link: [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)). I also confirmed this Dropout value of 0.2 as this is mentioned in the HuggingFace Distilled Bert config parameters for LM-based sequence classification task. \n","\n","# 3. **Optimizer, Learning Rate Scheduler And Loss Function Design**:\n","\n","*  Using recommendation in the previous paper and the recommendation guide by Huggingface on sequence classification, the optimizer used is Adam. The learning rate decays linearly from a maximum value of 5e-5. [Huggingface article link on how to train a sequence classifier.](https://huggingface.co/transformers/training.html/) \n","\n","* Since this is a classification problem, standard PyTorch CrossEntropyLoss is used as the loss function.\n","\n","# 4. **Training and Validation Loop:**\n","\n","* It made sense to freeze the languange model and just train the hidden layer parameters for the fine tuning task, which is a standard machine learning policy. But it is mentioned in the HuggingFace sequence classification guide that entire model fine-tuning gives better results.\n","\n","    > \"Note that if you are used to freezing the body of your pretrained model (like in computer vision) the above may seem a bit strange, as we are directly fine-tuning the whole model without taking any precaution. It actually works better this way for Transformers model (so this is not an oversight on our side. If you’re not familiar with what “freezing the body” of the model means, forget you read this paragraph.\"\n","  \n","* For training the following steps are used:\n","\n","\n","```\n","       1. Read one batch of tokenized reviews and labels from the train dataloader. \n","       2. Convert these values to the GPU.\n","       3. Zero the accumulated gradients (PyTorch limitation).\n","       4. Run model to get the predictions.\n","       5. Calculate the Loss.\n","       6. Compute back-propagation on the loss.\n","       7. Update the parameters.\n","       8. Update the learning rate.\n","       9. Repeat this process for each batch in the training dataloader (Covers the entire training space.)\n","       10. This is 1 epoch of trining. Calculate the F1 score and accuracy metrics on the validation set. Here, I use only 1600 samples of validation set to save time.\n","       11. Repeat this proess for 5 epochs. \n","       12. Save the model after this process concludes.\n","\n","```\n","It was noted that the loss for each batch of training samples falls to the order of 1e-4 for the fifth epoch.\n","\n","# 5. **Performace of the final model on Test, Validation and Training Sets:**\n","\n","* I have used sklearn to evaluate the precision, recall and F1 scores. The saved model is loaded and run on the entire test set to calculate F1 score. the results predictions are calculates and appended to a List. This list is finally converted to CPU and then sklearn API is called on the prediction and labels list to calculate Precision, Recall, F1 score and Accuracy.\n","\n","Now, I will explain code in each section and put the important results in this PDF.\n","\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ElAwGFGBfUPK"},"source":["## Install Huggingface Dependencies. "]},{"cell_type":"code","metadata":{"id":"ieoHXEWtuhgM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633300144673,"user_tz":240,"elapsed":6764,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"6f9adf40-c2df-43f8-c637-7c02e07a030d"},"source":["! pip install tokenizers\n","! pip install transformers"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.3)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"MZE0iRcmn3A_"},"source":["## Mount GDrive "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Taj8ByFGle85","executionInfo":{"status":"ok","timestamp":1633300134672,"user_tz":240,"elapsed":102,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"c80e8fdd-4580-4b21-e67d-c5d6c1ea030d"},"source":["# Mount Google Drive to this notebook\n","# The purpose is to allow your code to access to your files\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvtyjhhbmVUp","executionInfo":{"status":"ok","timestamp":1633300147937,"user_tz":240,"elapsed":114,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"13e3bfad-8fed-4cb9-8c7f-dc902c3c325e"},"source":["cd drive/MyDrive/nlp_assignments/assignment1/"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/nlp_assignments/assignment1\n"]}]},{"cell_type":"code","metadata":{"id":"HOnxb-Admch1","executionInfo":{"status":"ok","timestamp":1633300149588,"user_tz":240,"elapsed":1,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}}},"source":["import pandas as pd\n","import numpy as np"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NQ8mTDqS34z-"},"source":["### Load the Dataset as pandas df and calculate average word size for each review in the training dataset. This will help us in setting the maximum word size for the bert tokenizer."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S0BdEcZtm88g","executionInfo":{"status":"ok","timestamp":1633302180081,"user_tz":240,"elapsed":1623,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"3899399f-2ecb-42ac-dd61-b0cf39e85260"},"source":["train_df=pd.read_csv('/content/drive/My Drive/nlp_assignments/assignment1/imdbdataset/Train.csv')\n","test_df=pd.read_csv('/content/drive/My Drive/nlp_assignments/assignment1/imdbdataset/Test.csv')\n","val_df=pd.read_csv('/content/drive/My Drive/nlp_assignments/assignment1/imdbdataset/Valid.csv')\n","print(train_df.shape)\n","print(test_df.shape)\n","print(val_df.shape)\n","# print(train_df[:5])\n","print(train_df['text'].size)\n","words=0\n","for i in range(train_df['text'].size):\n","  words+=len(train_df['text'][i].split())\n","print('avg words in training dataset: ',words/train_df['text'].size)\n","print(train_df[0:5])"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["(40000, 2)\n","(5000, 2)\n","(5000, 2)\n","40000\n","avg words in training dataset:  231.33925\n","                                                text  label\n","0  I grew up (b. 1965) watching and loving the Th...      0\n","1  When I put this movie in my DVD player, and sa...      0\n","2  Why do people who do not know what a particula...      0\n","3  Even though I have great interest in Biblical ...      0\n","4  Im a die hard Dads Army fan and nothing will e...      1\n"]}]},{"cell_type":"markdown","metadata":{"id":"uSexqA_ZotQf"},"source":["###Load the pretrained Tokenizer. This will be the distilled bert cased."]},{"cell_type":"code","metadata":{"id":"wmBlsJ8rt9Mg","executionInfo":{"status":"ok","timestamp":1633300160164,"user_tz":240,"elapsed":1002,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}}},"source":["from transformers import BertTokenizer, BertModel,DistilBertTokenizer,DistilBertModel\n","encoder = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tbFRdGpmAZ4Z"},"source":["### Visualize encodings. This will be present in our dataloader getitem function."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QrfqYpZ25mmV","executionInfo":{"status":"ok","timestamp":1633300163046,"user_tz":240,"elapsed":132,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"46231d99-157c-414f-9bda-d0d1013ee783"},"source":["encodings=encoder(train_df['text'][1], add_special_tokens=True, padding='max_length', max_length=256,truncation=True)\n","encodings"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 1332, 146, 1508, 1142, 2523, 1107, 1139, 4173, 1591, 117, 1105, 2068, 1205, 1114, 170, 1884, 2391, 1105, 1199, 13228, 117, 146, 1125, 1199, 11471, 119, 146, 1108, 4717, 1115, 1142, 2523, 1156, 4651, 1199, 1104, 1103, 2012, 118, 1827, 1104, 1103, 1148, 2523, 131, 138, 10732, 6758, 8794, 117, 1363, 8342, 1642, 117, 6548, 1490, 2641, 117, 6276, 3789, 1105, 170, 5642, 118, 3919, 5945, 119, 1252, 117, 1106, 1139, 10866, 117, 1136, 1251, 1104, 1142, 1110, 1106, 1129, 1276, 1107, 17793, 131, 22644, 112, 188, 11121, 119, 6467, 146, 2373, 1199, 3761, 1148, 117, 146, 1547, 1136, 1138, 1151, 1177, 1519, 1205, 119, 1109, 1378, 24950, 1209, 1129, 2002, 1106, 1343, 1150, 1138, 1562, 1103, 1148, 2523, 117, 1105, 1150, 4927, 1122, 3120, 1111, 1103, 1827, 3025, 119, 133, 9304, 120, 135, 133, 9304, 120, 135, 1332, 1103, 1148, 2741, 2691, 117, 1240, 1107, 1111, 170, 4900, 1191, 1128, 1198, 3015, 17793, 131, 22644, 112, 188, 11121, 1121, 1103, 3934, 118, 1692, 1120, 1240, 1469, 6581, 23414, 113, 1137, 3451, 114, 117, 1105, 1125, 1103, 11471, 146, 1125, 119, 1109, 1390, 5115, 1112, 170, 2213, 24190, 1104, 1103, 1148, 2523, 117, 1105, 1103, 1490, 2641, 1144, 1151, 2125, 1118, 170, 1136, 1177, 11732, 1141, 119, 113, 1556, 1103, 5856, 1104, 170, 1374, 2650, 117, 1176, 1103, 1490, 1104, 7643, 114, 119, 1109, 4315, 9484, 2762, 1204, 1115, 2213, 117, 1133, 1103, 8794, 1107, 2440, 1110, 170, 6782, 3617, 119, 1109, 9844, 1110, 1145, 2785, 4780, 117, 1112, 1157, 1167, 1176, 1210, 3426, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"UmC5nscyGkJz"},"source":["### Dataloader design:\n","##### For each item(__getitem__) we will return the tokenized encodings. The length operator will be standard.\n"]},{"cell_type":"code","metadata":{"id":"1VgGjdOyAYnY","executionInfo":{"status":"ok","timestamp":1633300165056,"user_tz":240,"elapsed":111,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}}},"source":["from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","class imdbdataset(Dataset):\n","  def __init__(self, df, encoder):\n","    self.text=df['text']\n","    self.labels=df['label']\n","    self.encoder=encoder\n","  \n","  def __len__(self):\n","    return self.text.size\n","\n","  def __getitem__(self,item):\n","    text=self.text[item]\n","    label=self.labels[item]\n","    encoding=encoder(text, add_special_tokens=True, padding='max_length', max_length=256,truncation=True, return_tensors=\"pt\")\n","    return encoding['input_ids'].squeeze(0), encoding['attention_mask'].squeeze(0),label\n","  \n","\n","\n","\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3MgakSwFMvVi"},"source":["### Launch the Training and validation datset and dataloader"]},{"cell_type":"code","metadata":{"id":"hedGdPLxMt8Q","executionInfo":{"status":"ok","timestamp":1633302186345,"user_tz":240,"elapsed":123,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}}},"source":["train_dataset=imdbdataset(train_df, encoder)\n","train_dataloader=DataLoader(train_dataset, batch_size=16)\n","val_dataset=imdbdataset(val_df, encoder)\n","val_dataloader=DataLoader(val_dataset, batch_size=16)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bL1mAI9ApfYK"},"source":["### Visualize one sample of outputs from the dataloader. Input Ids are IDs for each word, which will be extended to 256. The attention mask for reviews which are less than 256 will be 0. 16 items in each batch."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZ5EjKMKNudo","executionInfo":{"status":"ok","timestamp":1633300171006,"user_tz":240,"elapsed":265,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"ef59e9fe-9aa7-49ee-cc01-a0d220655a7f"},"source":["inputids, attentionmasks, label=next(iter(train_dataloader))\n","print(inputids)\n","print(attentionmasks)\n","print(label)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 101,  146, 2580,  ...,    0,    0,    0],\n","        [ 101, 1332,  146,  ..., 1210, 3426,  102],\n","        [ 101, 2009, 1202,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 6155, 3635,  ...,    0,    0,    0],\n","        [ 101,  138, 1544,  ...,    0,    0,    0],\n","        [ 101, 1409, 1128,  ...,    0,    0,    0]])\n","tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]])\n","tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1])\n"]}]},{"cell_type":"markdown","metadata":{"id":"w22zhy6gX3ft"},"source":["### Load the model. Here the distilled lightweight version of bert will be used. Here, I checked the model configuration and found out that each tken has a size of 768 in the hidden layer and the sequence classification dropout is suggested as 0.2. The sape of CLS token is also printed in this cell."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511,"referenced_widgets":["4f9929c98c03455786efff8b9ae31408","60e8884e68304c6ca2e1769ceb5b1570","68f84527e5124dcbaa44315df71ddd28","a4dc97ec774b40acacd57594f672b5bd","78a0a9b35c6e4758a86cb1d93b7d3f84","51a3fc0e6e5f492d9a3a9ff32f3b0034","1e329cd317394d55bc51c682d7c4aa4c","14568195bfe246f5978d6b761a3f0335","1fb90a44d8f243ebbc7270afc52cb74f","0fac08a2358446fe8f9427a25d7f6c39","17301e970d8e4f259cee2cc4e9ebcad5"]},"id":"-9juuZ-TN0iW","executionInfo":{"status":"ok","timestamp":1633300200825,"user_tz":240,"elapsed":17270,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"9883526b-1db1-4f90-e7e8-2137adcf6aba"},"source":["model = DistilBertModel.from_pretrained('distilbert-base-cased')\n","print(model.config)\n","hidden_state=model(inputids,attentionmasks)\n","print(hidden_state[0][:,0,:].shape)"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f9929c98c03455786efff8b9ae31408","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/251M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-cased\",\n","  \"activation\": \"gelu\",\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.11.2\",\n","  \"vocab_size\": 28996\n","}\n","\n","torch.Size([16, 768])\n"]}]},{"cell_type":"markdown","metadata":{"id":"rLyXsOGBq2va"},"source":["### This is the Sentence Classifier Class. In the forward pass, the CLS token in distilled version of Bert is passed to a linear layer with hidden size 20 and a dropout of 0.2, a ReLU, and another Linear Layer of 2 output dimentions(1 for positive and 1 for negative)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wSZbTw5xs9g","executionInfo":{"status":"ok","timestamp":1633300203736,"user_tz":240,"elapsed":109,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"b8e8ac31-3259-4e5d-a80d-274f8f763556"},"source":["import torch\n","import torch.nn as nn\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","class sentence_classifier(nn.Module):\n","  def __init__(self):\n","    super(sentence_classifier,self).__init__()\n","    self.DBert = DistilBertModel.from_pretrained('distilbert-base-cased')\n","    self.classifier = nn.Sequential(\n","          nn.Linear(768, 20),\n","          nn.ReLU(),\n","          nn.Dropout(0.2),\n","          nn.Linear(20, 2)\n","        )\n","  def forward(self, input_ids, attention_masks):\n","    hidden_state=self.DBert(input_ids,attention_masks)\n","    cls=hidden_state[0][:,0,:]\n","    logits=self.classifier(cls)\n","    return logits\n","# d_model=model.to(device)"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"markdown","metadata":{"id":"vpl6bS0LrnHV"},"source":["### This initialization function initialises the model to the sentence classifier class, the optimizer to standard Adam, and the learning rate scheduler to linearly decay from 5e-5. All 3 objects are returned."]},{"cell_type":"code","metadata":{"id":"v3gsLAmQKoHY"},"source":["from transformers import AdamW\n","from transformers import get_scheduler\n","def init_model():\n","  imdb_classifier=sentence_classifier()\n","  imdb_classifier.to(device)\n","  optimizer = AdamW(imdb_classifier.parameters(), lr=5e-5)\n","  num_epochs = 5\n","  num_training_steps = num_epochs * len(train_dataloader)\n","  print(num_training_steps)\n","  lr_scheduler = get_scheduler(\n","      \"linear\",\n","      optimizer=optimizer,\n","      num_warmup_steps=0,\n","      num_training_steps=num_training_steps\n","  )\n","  return imdb_classifier,optimizer,lr_scheduler"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"86nV2cmusR-X"},"source":["### Initialize the loss function to Cross Entropy Loss since this is a classification problem."]},{"cell_type":"code","metadata":{"id":"kOxdLlsjW0C6"},"source":["loss_fn=nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BqO7C_25scU7"},"source":["### Use sklearn to compute metrics. This function expects 2 lists, and prints the Precision, Recall, F1 Score and Accuracy values: \n","\n","1.   Prediction List: Output of the model, with the prediction label as 1 for the predicted class.\n","2.   Labels List: Correct labels from the dataloader class. \n","\n","**Note: This operation is performed on the CPU and not the GPU, hence the lists are passed to the CPU before calling this function.**\n","\n"]},{"cell_type":"code","metadata":{"id":"AiKOeUd_FA38","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633300222023,"user_tz":240,"elapsed":3469,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"43300bf7-7624-4118-ddde-d555230f5b8a"},"source":["!pip install sklearn\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import accuracy_score\n","def compute_metrics(preds,labels):\n","  print(preds,labels)\n","  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n","  acc = accuracy_score(labels, preds)\n","  print(\"Precision = \", precision)\n","  print(\"recall = \", recall)\n","  print(\"F1 score = \", f1)\n","  print(\"accuracy = \", acc)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"XyssW5wftxKH"},"source":["### Training Loop. The batch loss (Which is the Cross Entropy Loss added for 16 samples) is printed after every 1600 training samples. After every epoch, a validation cycle is performed on 1600 samples, and the result metrics are printed. There is a scope to perform grid search and tune the hyperparameters here, but the results generated after the first epoch are satisfactory."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GVQUg9dQyqLq","executionInfo":{"status":"ok","timestamp":1633215185386,"user_tz":240,"elapsed":9427567,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"df9d37a2-6f67-4e9a-f1d1-0ed651c1e29a"},"source":["import itertools\n","imdb_model,optimizer,lr_scheduler=init_model()\n","imdb_model.train()\n","num_epochs=5\n","for epoch in range(num_epochs):\n","  rev_predictions=[]\n","  rev_labels=[]\n","  iter=0\n","  for input_ids, attentionmasks,labels in train_dataloader:\n","    iter+=1\n","    d_inputids=input_ids.to(device)\n","    d_attentionmasks=attentionmasks.to(device)\n","    d_labels=labels.to(device)\n","    imdb_model.zero_grad()\n","    logits = imdb_model(d_inputids, d_attentionmasks)\n","    loss=loss_fn(logits, d_labels)\n","    if (iter%99==0):\n","      print(iter)\n","      print(loss)\n","    loss.backward()\n","    optimizer.step()\n","    lr_scheduler.step()\n","  print(\"1 epoch terminated. Evaluating on validation....\")\n","  imdb_model.eval()\n","  for input_ids, attentionmasks,labels in itertools.islice(val_dataloader,100):\n","    d_inputids=input_ids.to(device)\n","    d_attentionmasks=attentionmasks.to(device)\n","    d_labels=labels.to(device)\n","    with torch.no_grad():\n","      logits = imdb_model(d_inputids, d_attentionmasks)\n","    predictions = torch.argmax(logits, dim=-1)\n","    rev_predictions.extend(predictions)\n","    rev_labels.extend(d_labels)\n","  compute_metrics(torch.stack(rev_predictions).cpu(), torch.stack(rev_labels).cpu())  \n","# print(rev_labels,rev_predictions)\n","# torch.save(imdb_model.state_dict(),'/content/drive/My Drive/nlp_assignments/assignment1/pytorch_model_distillbert.bin')\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["12500\n","99\n","tensor(0.3428, device='cuda:0', grad_fn=<NllLossBackward>)\n","198\n","tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n","297\n","tensor(0.4405, device='cuda:0', grad_fn=<NllLossBackward>)\n","396\n","tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n","495\n","tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n","594\n","tensor(0.5980, device='cuda:0', grad_fn=<NllLossBackward>)\n","693\n","tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n","792\n","tensor(0.3631, device='cuda:0', grad_fn=<NllLossBackward>)\n","891\n","tensor(0.8403, device='cuda:0', grad_fn=<NllLossBackward>)\n","990\n","tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n","1089\n","tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n","1188\n","tensor(0.3393, device='cuda:0', grad_fn=<NllLossBackward>)\n","1287\n","tensor(0.3542, device='cuda:0', grad_fn=<NllLossBackward>)\n","1386\n","tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n","1485\n","tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n","1584\n","tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n","1683\n","tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n","1782\n","tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n","1881\n","tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n","1980\n","tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n","2079\n","tensor(0.4836, device='cuda:0', grad_fn=<NllLossBackward>)\n","2178\n","tensor(0.2015, device='cuda:0', grad_fn=<NllLossBackward>)\n","2277\n","tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n","2376\n","tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n","2475\n","tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n","1 epoch terminated. Evaluating on validation....\n","tensor([0, 0, 0,  ..., 0, 1, 0]) tensor([0, 0, 0,  ..., 0, 1, 0])\n","Precision =  0.9252577319587629\n","recall =  0.8831488314883149\n","F1 score =  0.9037130270610447\n","accuracy =  0.904375\n","99\n","tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n","198\n","tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n","297\n","tensor(0.3258, device='cuda:0', grad_fn=<NllLossBackward>)\n","396\n","tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n","495\n","tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n","594\n","tensor(0.3444, device='cuda:0', grad_fn=<NllLossBackward>)\n","693\n","tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward>)\n","792\n","tensor(0.2628, device='cuda:0', grad_fn=<NllLossBackward>)\n","891\n","tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward>)\n","990\n","tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n","1089\n","tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n","1188\n","tensor(0.2935, device='cuda:0', grad_fn=<NllLossBackward>)\n","1287\n","tensor(0.5236, device='cuda:0', grad_fn=<NllLossBackward>)\n","1386\n","tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n","1485\n","tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n","1584\n","tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n","1683\n","tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n","1782\n","tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n","1881\n","tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n","1980\n","tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n","2079\n","tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>)\n","2178\n","tensor(0.1703, device='cuda:0', grad_fn=<NllLossBackward>)\n","2277\n","tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n","2376\n","tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n","2475\n","tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n","1 epoch terminated. Evaluating on validation....\n","tensor([0, 0, 0,  ..., 0, 1, 0]) tensor([0, 0, 0,  ..., 0, 1, 0])\n","Precision =  0.9427430093209055\n","recall =  0.8708487084870848\n","F1 score =  0.9053708439897697\n","accuracy =  0.9075\n","99\n","tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n","198\n","tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n","297\n","tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n","396\n","tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n","495\n","tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n","594\n","tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n","693\n","tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n","792\n","tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n","891\n","tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n","990\n","tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n","1089\n","tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward>)\n","1188\n","tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n","1287\n","tensor(0.3872, device='cuda:0', grad_fn=<NllLossBackward>)\n","1386\n","tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n","1485\n","tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n","1584\n","tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n","1683\n","tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n","1782\n","tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n","1881\n","tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n","1980\n","tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n","2079\n","tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n","2178\n","tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n","2277\n","tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n","2376\n","tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n","2475\n","tensor(0.2817, device='cuda:0', grad_fn=<NllLossBackward>)\n","1 epoch terminated. Evaluating on validation....\n","tensor([0, 0, 0,  ..., 0, 1, 0]) tensor([0, 0, 0,  ..., 0, 1, 0])\n","Precision =  0.9388816644993498\n","recall =  0.8880688806888068\n","F1 score =  0.9127686472819216\n","accuracy =  0.91375\n","99\n","tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n","198\n","tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n","297\n","tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n","396\n","tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n","495\n","tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n","594\n","tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n","693\n","tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n","792\n","tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n","891\n","tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n","990\n","tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n","1089\n","tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n","1188\n","tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n","1287\n","tensor(0.4181, device='cuda:0', grad_fn=<NllLossBackward>)\n","1386\n","tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n","1485\n","tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n","1584\n","tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n","1683\n","tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n","1782\n","tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n","1881\n","tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n","1980\n","tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n","2079\n","tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n","2178\n","tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n","2277\n","tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n","2376\n","tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n","2475\n","tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n","1 epoch terminated. Evaluating on validation....\n","tensor([0, 0, 0,  ..., 0, 1, 0]) tensor([0, 0, 0,  ..., 0, 1, 0])\n","Precision =  0.9109756097560976\n","recall =  0.9188191881918819\n","F1 score =  0.9148805878750766\n","accuracy =  0.913125\n","99\n","tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n","198\n","tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n","297\n","tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n","396\n","tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n","495\n","tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n","594\n","tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n","693\n","tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n","792\n","tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n","891\n","tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n","990\n","tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n","1089\n","tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n","1188\n","tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n","1287\n","tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward>)\n","1386\n","tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n","1485\n","tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n","1584\n","tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n","1683\n","tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n","1782\n","tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n","1881\n","tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n","1980\n","tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n","2079\n","tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n","2178\n","tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n","2277\n","tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n","2376\n","tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n","2475\n","tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n","1 epoch terminated. Evaluating on validation....\n","tensor([0, 0, 0,  ..., 0, 1, 0]) tensor([0, 0, 0,  ..., 0, 1, 0])\n","Precision =  0.9062119366626066\n","recall =  0.915129151291513\n","F1 score =  0.9106487148102816\n","accuracy =  0.90875\n"]}]},{"cell_type":"markdown","metadata":{"id":"v_klBtC4unPf"},"source":["### This Command saves the trained model."]},{"cell_type":"code","metadata":{"id":"wqpCWQ6SVRue"},"source":["torch.save(imdb_model,'/content/drive/My Drive/nlp_assignments/assignment1/pytorch_model_distillbert.bin')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ub1rVbzt0vFH"},"source":["### Check accuracy and F1 score on the Test Set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L6E5xZdXztUX","executionInfo":{"status":"ok","timestamp":1633302130465,"user_tz":240,"elapsed":78719,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"5749fbf7-1f01-4c03-d2ab-a44309978879"},"source":["test_dataset=imdbdataset(test_df, encoder)\n","test_dataloader=DataLoader(test_dataset, batch_size=16)\n","num_iters=16\n","load_model= torch.load('/content/drive/My Drive/nlp_assignments/assignment1/pytorch_model_distillbert.bin')\n","load_model.to(device)\n","import itertools\n","load_model.eval()\n","rev_predictions=[]\n","rev_labels=[]\n","for input_ids, attentionmasks,labels in test_dataloader:\n","  d_inputids=input_ids.to(device)\n","  d_attentionmasks=attentionmasks.to(device)\n","  d_labels=labels.to(device)\n","  with torch.no_grad():\n","    outputs = load_model(d_inputids, d_attentionmasks)\n","  predictions = torch.argmax(outputs, dim=-1)\n","  rev_predictions.extend(predictions)\n","  rev_labels.extend(d_labels)\n","print(len(rev_predictions))\n","print(len(rev_labels))\n","compute_metrics(torch.stack(rev_predictions).cpu(), torch.stack(rev_labels).cpu())  "],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["5000\n","5000\n","tensor([0, 0, 0,  ..., 0, 0, 0]) tensor([0, 0, 0,  ..., 0, 0, 0])\n","Precision =  0.9021823850350741\n","recall =  0.9241516966067864\n","F1 score =  0.9130349043581149\n","accuracy =  0.9118\n"]}]},{"cell_type":"markdown","metadata":{"id":"wujCKPvMuxfE"},"source":["### Check accuracy and F1 score on the Validation set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zNt1-DrhhSUE","executionInfo":{"status":"ok","timestamp":1633237474578,"user_tz":240,"elapsed":84440,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"3db78d0e-860f-42d2-833c-9c0eba86e6c9"},"source":["load_model= torch.load('/content/drive/My Drive/nlp_assignments/assignment1/pytorch_model_distillbert.bin')\n","load_model.to(device)\n","import itertools\n","load_model.eval()\n","rev_predictions=[]\n","rev_labels=[]\n","for input_ids, attentionmasks,labels in val_dataloader:\n","  d_inputids=input_ids.to(device)\n","  d_attentionmasks=attentionmasks.to(device)\n","  d_labels=labels.to(device)\n","  with torch.no_grad():\n","    outputs = load_model(d_inputids, d_attentionmasks)\n","  predictions = torch.argmax(outputs, dim=-1)\n","  rev_predictions.extend(predictions)\n","  rev_labels.extend(d_labels)\n","print(len(rev_predictions))\n","print(len(rev_labels))\n","compute_metrics(torch.stack(rev_predictions).cpu(), torch.stack(rev_labels).cpu())  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5000\n","5000\n","tensor([0, 0, 0,  ..., 1, 1, 1]) tensor([0, 0, 0,  ..., 1, 1, 1])\n","Precision =  0.9033646322378717\n","recall =  0.9184566428003182\n","F1 score =  0.9108481262327416\n","accuracy =  0.9096\n"]}]},{"cell_type":"markdown","metadata":{"id":"41YGD9Hyu1g4"},"source":["### Check accuracy and F1 score on the Train Set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4VOXD9ZjXpq","executionInfo":{"status":"ok","timestamp":1633302820633,"user_tz":240,"elapsed":619115,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"b96d9cf4-5ec2-4cc9-c77c-99ed08d5649b"},"source":["load_model= torch.load('/content/drive/My Drive/nlp_assignments/assignment1/pytorch_model_distillbert.bin')\n","load_model.to(device)\n","import itertools\n","load_model.eval()\n","rev_predictions=[]\n","rev_labels=[]\n","for input_ids, attentionmasks,labels in train_dataloader:\n","  d_inputids=input_ids.to(device)\n","  d_attentionmasks=attentionmasks.to(device)\n","  d_labels=labels.to(device)\n","  with torch.no_grad():\n","    outputs = load_model(d_inputids, d_attentionmasks)\n","  predictions = torch.argmax(outputs, dim=-1)\n","  rev_predictions.extend(predictions)\n","  rev_labels.extend(d_labels)\n","print(len(rev_predictions))\n","print(len(rev_labels))\n","compute_metrics(torch.stack(rev_predictions).cpu(), torch.stack(rev_labels).cpu())  "],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["40000\n","40000\n","tensor([0, 0, 0,  ..., 0, 1, 1]) tensor([0, 0, 0,  ..., 0, 1, 1])\n","Precision =  0.9993494144730257\n","recall =  0.9993994294579851\n","F1 score =  0.9993744213397393\n","accuracy =  0.999375\n"]}]},{"cell_type":"code","metadata":{"id":"0a82NzwK2wgQ"},"source":["!apt-get -qq install texlive texlive-xetex texlive-latex-extra pandoc\n","!pip install --quiet pypandoc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2rSRLRQTl8gA","outputId":"2a103de0-aeb0-453e-f327-4dc7a33d3c4a"},"source":["!jupyter nbconvert --to PDF \"/content/drive/MyDrive/Colab Notebooks/Assignment1_apoorvgarg_db.ipynb\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab Notebooks/Assignment1_apoorvgarg_db.ipynb to PDF\n"]}]},{"cell_type":"markdown","metadata":{"id":"dyp_mn4pOGnO"},"source":["# Section 1 : Inference Block. Run Inference on this block. Just chcange the paths of model(line 15) and data csv reader(line 16). Please remove the drive mount command(line 6) if the data and model is not shared using google drive. On running the block, the accuracy, precision, recall and F1 score metrics will be printed in the stdout block.."]},{"cell_type":"code","metadata":{"id":"621Ci8fXvV6w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633302940384,"user_tz":240,"elapsed":89565,"user":{"displayName":"Apoorv Garg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13442906426426063638"}},"outputId":"31efda4e-4a22-4d67-e0a9-104b0df43e0f"},"source":["! pip install tokenizers\n","! pip install transformers\n","!pip install sklearn\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","import numpy as np\n","from transformers import BertTokenizer, BertModel,DistilBertTokenizer,DistilBertModel\n","import torch\n","import torch.nn as nn\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import accuracy_score\n","model_path='/content/drive/MyDrive/nlp_assignments/assignment1/pytorch_model_distillbert.bin'\n","test_file_path='/content/drive/My Drive/nlp_assignments/assignment1/imdbdataset/Test.csv'\n","test_df=pd.read_csv(test_file_path)\n","encoder = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n","class imdbdataset(Dataset):\n","  def __init__(self, df, encoder):\n","    self.text=df['text']\n","    self.labels=df['label']\n","    self.encoder=encoder\n","  \n","  def __len__(self):\n","    return self.text.size\n","\n","  def __getitem__(self,item):\n","    text=self.text[item]\n","    label=self.labels[item]\n","    encoding=encoder(text, add_special_tokens=True, padding='max_length', max_length=256,truncation=True, return_tensors=\"pt\")\n","    return encoding['input_ids'].squeeze(0), encoding['attention_mask'].squeeze(0),label\n","\n","def compute_metrics(preds,labels):\n","  print(preds,labels)\n","  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n","  acc = accuracy_score(labels, preds)\n","  print(\"Precision = \", precision)\n","  print(\"recall = \", recall)\n","  print(\"F1 score = \", f1)\n","  print(\"accuracy = \", acc)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","test_dataset=imdbdataset(test_df, encoder)\n","test_dataloader=DataLoader(test_dataset, batch_size=16)\n","class sentence_classifier(nn.Module):\n","  def __init__(self):\n","    super(sentence_classifier,self).__init__()\n","    self.DBert = DistilBertModel.from_pretrained('distilbert-base-cased')\n","    self.classifier = nn.Sequential(\n","          nn.Linear(768, 20),\n","          nn.ReLU(),\n","          nn.Dropout(0.2),\n","          nn.Linear(20, 2)\n","        )\n","  def forward(self, input_ids, attention_masks):\n","    hidden_state=self.DBert(input_ids,attention_masks)\n","    cls=hidden_state[0][:,0,:]\n","    logits=self.classifier(cls)\n","    return logits\n","load_model= torch.load(model_path)\n","load_model.to(device)\n","import itertools\n","load_model.eval()\n","rev_predictions=[]\n","rev_labels=[]\n","for input_ids, attentionmasks,labels in test_dataloader:\n","  d_inputids=input_ids.to(device)\n","  d_attentionmasks=attentionmasks.to(device)\n","  d_labels=labels.to(device)\n","  with torch.no_grad():\n","    outputs = load_model(d_inputids, d_attentionmasks)\n","  predictions = torch.argmax(outputs, dim=-1)\n","  rev_predictions.extend(predictions)\n","  rev_labels.extend(d_labels)\n","print(len(rev_predictions))\n","print(len(rev_labels))\n","compute_metrics(torch.stack(rev_predictions).cpu(), torch.stack(rev_labels).cpu())  "],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.3)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","5000\n","5000\n","tensor([0, 0, 0,  ..., 0, 0, 0]) tensor([0, 0, 0,  ..., 0, 0, 0])\n","Precision =  0.9021823850350741\n","recall =  0.9241516966067864\n","F1 score =  0.9130349043581149\n","accuracy =  0.9118\n"]}]},{"cell_type":"code","metadata":{"id":"TGdE1qyHPaZ2"},"source":[""],"execution_count":null,"outputs":[]}]}